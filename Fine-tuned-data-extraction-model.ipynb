{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729bded-cd25-4486-9c2b-defbb47845a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b06bea9f6846b2b2e6021d18ffc5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08be97d8b4ed437daa8327ba56975374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/3186 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FINE-TUNED MODEL\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_supaport, accuracy_score\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "dataset_path = r\"C:\\Users\\ASUS\\OneDrive\\Documents\\Semester 9\\Mini Project\\NER.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "df['BIO_tags'] = df['BIO_tags'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a mapping from labels to IDs and vice versa\n",
    "unique_labels = set(label for labels in df['BIO_tags'] for label in labels)\n",
    "label_to_id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "df['labels'] = df['BIO_tags'].apply(lambda x: [label_to_id[label] for label in x])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['tokens', 'labels']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['tokens', 'labels']])\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100) \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  \n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "num_labels = len(label_to_id)\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    logging_steps=50,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./ner_model')\n",
    "tokenizer.save_pretrained('./ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8321a06c-dd98-4386-8a49-5cb8d114d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\new_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import docx\n",
    "import torch\n",
    "import queue\n",
    "import PyPDF2\n",
    "import logging\n",
    "import requests\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from sklearn.svm import SVC\n",
    "from bs4 import BeautifulSoup\n",
    "from tkinter import filedialog, messagebox, Text\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline, BertTokenizerFast, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc75f5d-d5e6-4a08-93ee-c7f00f5fd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fine-tuned model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./ner_model')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./ner_model')\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02d0999-ccdc-46d4-bb97-2ed2152dad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a2144b-4d43-4429-80f7-70341ff5a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file():\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Case File\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\"), (\"Word Documents\", \"*.docx\"), (\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        threading.Thread(target=process_file, args=(file_path,), daemon=True).start()\n",
    "        start_progress_bar()\n",
    "    else:\n",
    "        messagebox.showinfo(\"Info\", \"No file selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2669c9d-5c5c-4cf6-b18e-8ffd489df580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the file and printing necessary outputs\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        logging.info(f\"File Selected: {file_path}\")\n",
    "        file_label.config(text=f\"Selected File Path: {file_path}\")\n",
    "\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            content = extract_pdf(file_path)\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            content = extract_docx(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            content = extract_txt(file_path)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Unsupported file format.\")\n",
    "            return\n",
    "\n",
    "        entities = extract_entities(content)\n",
    "        unique_entities = list({(ent['word'], ent['entity_group']) for ent in entities})\n",
    "        print(\"Extracted Entities:\", unique_entities)\n",
    "\n",
    "        search_query = \" \".join([ent[0] for ent in unique_entities])\n",
    "        case_links = search_indian_kanoon(search_query)\n",
    "        print(\"Found Case Links:\", case_links)\n",
    "\n",
    "        for i in range(min(len(unique_entities), len(case_links))):\n",
    "            ent = unique_entities[i]\n",
    "            link = case_links[i]\n",
    "            print(f\"{ent[0]} : {link}\")\n",
    "\n",
    "        result_queue.put(content)\n",
    "        root.after(100, update_gui)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        stop_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1e0652-9730-44c2-b295-79c4548c3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gui():\n",
    "    try:\n",
    "        while not result_queue.empty():\n",
    "            content = result_queue.get()\n",
    "            display_content(content)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while updating GUI: {e}\")\n",
    "    finally:\n",
    "        root.after(100, update_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11c3b04-be93-444f-b957-efd57364dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_content(content):\n",
    "    content_window = tk.Toplevel()\n",
    "    content_window.title(\"Extracted Content\")\n",
    "    text_area = Text(content_window, wrap='word')\n",
    "    text_area.insert('1.0', \"Extracted Content:\\n\\n\" + content + \"\\n\\n\")\n",
    "    text_area.config(state='disabled')\n",
    "    text_area.pack(expand=True, fill='both')\n",
    "    content_window.geometry(\"600x500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4779dec8-c5ea-4e59-870f-f68296b14afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(file_path):\n",
    "    print(f\"Processing PDF: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    content += text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba665d1-0582-4b8a-a346-a50a9b3e6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docx(file_path):\n",
    "    print(f\"Processing DOCX: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            content += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOCX: {e}\")\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435f41da-81ca-4e94-b620-41d85b26b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_txt(file_path):\n",
    "    print(f\"Processing TXT: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TXT: {e}\")\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1e2642-e720-4a0e-9188-37ef24c11109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    try:\n",
    "        entities = ner_pipeline(text)\n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c99e50-e416-4116-998a-67b4393209bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_indian_kanoon(query):\n",
    "    base_url = \"https://indiankanoon.org/search/?formInput=\"\n",
    "    search_url = base_url + query.replace(\" \", \"+\")\n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        case_links = []\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link['href']\n",
    "            if \"/doc/\" in href:\n",
    "                case_links.append(\"https://indiankanoon.org\" + href)\n",
    "        return case_links\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while searching: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace14139-3daa-4b59-b77f-e5632c2ac54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_progress_bar():\n",
    "    progress_bar.pack(pady=10)\n",
    "    progress_bar.start(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acda853f-3072-4fc4-b161-3cf03304c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_progress_bar():\n",
    "    progress_bar.stop()\n",
    "    progress_bar.pack_forget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "924a6d1c-7222-4b01-a1da-facc3f9a92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entities():\n",
    "    query = search_entry.get()\n",
    "    if query:\n",
    "        threading.Thread(target=process_search, args=(query,), daemon=True).start()\n",
    "        start_progress_bar()\n",
    "    else:\n",
    "        messagebox.showinfo(\"Info\", \"Please enter a search term.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e640aa84-dd2f-47c3-8497-82dbbf816923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_search_results(entities, case_links):\n",
    "    results_window = tk.Toplevel()\n",
    "    results_window.title(\"Search Results\")\n",
    "    results_window.geometry(\"600x500\")\n",
    "    text_area = Text(results_window, wrap='word')\n",
    "\n",
    "    content = \"Extracted Entities and Case Links:\\n\\n\"\n",
    "    for ent, link in zip(entities, case_links):\n",
    "        content += f\"{ent['word']} : {link}\\n\"\n",
    "    \n",
    "    # Display clickable links in text area\n",
    "    text_area.insert('1.0', content)\n",
    "    text_area.config(state='disabled')\n",
    "    text_area.pack(expand=True, fill='both')\n",
    "\n",
    "    for link in case_links:\n",
    "        text_area.insert(tk.END, f\"\\n{link}\\n\", 'hyperlink')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e6a781b-ef17-47ba-b6f3-8d748c92c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_search(query):\n",
    "    try:\n",
    "        entities = extract_entities(query)\n",
    "        print(\"Extracted Entities from Search Query:\", entities)\n",
    "\n",
    "        search_query = \" \".join([ent['word'] for ent in entities])\n",
    "        case_links = search_indian_kanoon(search_query)\n",
    "        print(\"Found Case Links:\", case_links)\n",
    "        content = \"Extracted Entities and Case Links:\\n\\n\"\n",
    "        for ent, link in zip(entities, case_links):\n",
    "            content += f\"{ent['word']} : {link}\\n\"\n",
    "        print(content)\n",
    "        \n",
    "        display_search_results(entities, case_links)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred during the search: {e}\")\n",
    "    finally:\n",
    "        stop_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b43581f-902e-4672-833f-08a59f38c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"File Upload and Content Extractor\")\n",
    "root.geometry(\"600x400\")\n",
    "root.config(bg=\"#f0f0f0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8817004-7690-4706-8f00-d5e9b16e05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_label = tk.Label(root, text=\"Upload the case file as .pdf, .docx or .txt\",\n",
    "                               bg=\"#f0f0f0\", font=(\"Arial\", 12))\n",
    "instruction_label.pack(pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6529662d-6a03-4bba-876d-827c8dcf7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry for search term\n",
    "search_entry = tk.Entry(root, width=50, font=(\"Arial\", 12))\n",
    "search_entry.pack(pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efde2b16-7b48-4038-9727-8675b5ecd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search button\n",
    "search_btn = tk.Button(root, text=\"Search\", command=search_entities, width=20, bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "search_btn.pack(pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901464c1-e72b-4c73-a54c-91c1d467ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame for the upload button\n",
    "frame = tk.Frame(root, bg=\"#ffffff\", bd=2, relief=tk.GROOVE)\n",
    "frame.pack(pady=20, padx=10, fill='both', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76165c4d-fa9f-4b15-af77-87b2dabefb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_btn = tk.Button(frame, text=\"Upload File\", command=upload_file, width=20, bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "upload_btn.pack(pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2020b2a-dbd9-4a26-b067-f5a4468ed95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label to display selected file path\n",
    "file_label = tk.Label(frame, text=\"Selected File Path: \", bg=\"#f0f0f0\", font=(\"Arial\", 10))\n",
    "file_label.pack(pady=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f4ff97f-3a0c-483d-b6a9-09c8258b5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar (initially hidden)\n",
    "progress_bar = ttk.Progressbar(root, orient=\"horizontal\", mode=\"indeterminate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ee36b2e-94a5-4f96-93f7-2ad358cf4005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Tkinter event loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56b859d6-d36b-4950-9562-76d61a428874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Case_Title             Citation  \\\n",
      "0    State Of Karnataka Vs State Of Karnataka (1986)  2013 BLA 1241 (KAR)   \n",
      "1  Karnataka Wakf Board vs. State of Karnataka an...   2003 BLA 1243 (SC)   \n",
      "2  Super Sales Corporation Vs Sr. Superintendent ...  1999 BLA 1246 (OHC)   \n",
      "3  Lakshmi Narayana Puli vs Indian Oil Corporatio...   2012 BLA 1248 (AP)   \n",
      "4      Union of India vs Anwar Ahemad Qureshi (1998)  1998 BLA 1249 (BOM)   \n",
      "\n",
      "                       Court Judgement_Date  \\\n",
      "0       Karnataka High Court       3/1/2013   \n",
      "1     Supreme Court of India     30-01-2003   \n",
      "2          Orissa High Court     16-01-1999   \n",
      "3  Andhra Pradesh High Court       7/1/2012   \n",
      "4          Bombay High Court     28-01-1998   \n",
      "\n",
      "                                              Judges  \\\n",
      "0          K.N. Keshavanarayana J, Ram Mohan Reddy J   \n",
      "1  S. Rajendra Babu J, K.G. Balakrishnan J, G.P. ...   \n",
      "2                          R.K. Patra J, R.K. Dash J   \n",
      "3                  Madan B. Lokur CJ, Sanjay Kumar J   \n",
      "4                            G.R. Bedge & A.A. Halbe   \n",
      "\n",
      "                                        Petitioner  \\\n",
      "0  P. Uma, P. Varsha, P. Vaishnavi, and Lalithamma   \n",
      "1                             Karnataka Wakf Board   \n",
      "2                          Super Sales Corporation   \n",
      "3                            Lakshmi Narayana Puli   \n",
      "4                                   Union of India   \n",
      "\n",
      "                                          Respondent           Bench  \\\n",
      "0                                  P. Uma and Others  Division Bench   \n",
      "1                     State of Karnataka and Another      Full Bench   \n",
      "2  Sr. Superintendent Of Post Offices, Bhubaneswa...  Division Bench   \n",
      "3                         Indian Oil Corporation Ltd  Division Bench   \n",
      "4                               Anwar Ahemad Qureshi  Division Bench   \n",
      "\n",
      "                                          Legal_Laws  \\\n",
      "0  Motor Vehicles Act, 1988: Governs claims for c...   \n",
      "1  Adverse Possession: The principle that continu...   \n",
      "2  Post Office Act, 1898, Section 3: Mandates del...   \n",
      "3  Article 226 of the Constitution of India: Allo...   \n",
      "4  Consumer Protection Act, 1986 Section 2(1): De...   \n",
      "\n",
      "                                           Preceding  \\\n",
      "0  Calculation of Compensation: The court reviewe...   \n",
      "1  Wakf Act, 1954: Section 6 of the Wakf Act prov...   \n",
      "2  Postal authoritiesâ€™ obligation to deliver arti...   \n",
      "3  Maneka Gandhi vs Union of India (1978) - Empha...   \n",
      "4  National Commission Precedent: The National Co...   \n",
      "\n",
      "                                              Result  \n",
      "0  Compensation under conventional heads such as ...  \n",
      "1  The Supreme Court allowed the appeals and rule...  \n",
      "2  Postal authorities were directed to deliver po...  \n",
      "3                                  Appeal dismissed.  \n",
      "4                                  Appeal dismissed.  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9de7e30ef74cd1b2b277ce7ad6ff49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbfc6746a8b40f48356d117ac34c883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "import asyncio\n",
    "import queue\n",
    "import logging\n",
    "import requests\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, Text\n",
    "import PyPDF2\n",
    "import docx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the cases CSV file\n",
    "cases_df = pd.read_csv(r\"C:\\Users\\ASUS\\OneDrive\\Documents\\Semester 9\\Mini Project\\Cases.csv\")\n",
    "\n",
    "# Check the CSV structure (make sure 'CaseName' and 'CaseDetails' columns exist)\n",
    "print(cases_df.head())\n",
    "\n",
    "dataset_path = r\"C:\\Users\\ASUS\\OneDrive\\Documents\\Semester 9\\Mini Project\\NER.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "df['BIO_tags'] = df['BIO_tags'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a mapping from labels to IDs and vice versa\n",
    "unique_labels = set(label for labels in df['BIO_tags'] for label in labels)\n",
    "label_to_id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "df['labels'] = df['BIO_tags'].apply(lambda x: [label_to_id[label] for label in x])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['tokens', 'labels']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['tokens', 'labels']])\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100) \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  \n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "num_labels = len(label_to_id)\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    logging_steps=50,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "tokenizer.save_pretrained('./ner_model')\n",
    "\n",
    "# Loading the fine-tuned model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./ner_model')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./ner_model')\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# Function to upload case files\n",
    "def upload_file():\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Case File\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\"), (\"Word Documents\", \"*.docx\"), (\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        threading.Thread(target=process_file, args=(file_path,), daemon=True).start()\n",
    "        start_progress_bar()\n",
    "    else:\n",
    "        messagebox.showinfo(\"Info\", \"No file selected.\")\n",
    "\n",
    "# Processing the file and printing necessary outputs\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        logging.info(f\"File Selected: {file_path}\")\n",
    "        file_label.config(text=f\"Selected File Path: {file_path}\")\n",
    "\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            content = extract_pdf(file_path)\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            content = extract_docx(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            content = extract_txt(file_path)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Unsupported file format.\")\n",
    "            return\n",
    "\n",
    "        # Extract entities from content\n",
    "        entities = extract_entities(content)\n",
    "        unique_entities = list({(ent['word'], ent['entity_group']) for ent in entities})\n",
    "        print(\"Extracted Entities:\", unique_entities)\n",
    "\n",
    "        # Search for these entities in the Cases CSV file\n",
    "        case_matches = []\n",
    "        for ent in unique_entities:\n",
    "            word = ent[0]  # The entity word\n",
    "            # Check for the entity word in the 'CaseName' or 'CaseDetails' column\n",
    "            matched_cases = cases_df[cases_df['CaseName'].str.contains(word, case=False, na=False)]\n",
    "            case_matches.extend(matched_cases[['CaseName', 'CaseDetails']].values.tolist())\n",
    "\n",
    "        if case_matches:\n",
    "            print(\"Matched Cases:\")\n",
    "            for case in case_matches:\n",
    "                print(f\"Case Name: {case[0]}, Case Details: {case[1]}\")\n",
    "\n",
    "            # Display matched cases in the GUI\n",
    "            result_queue.put(case_matches)\n",
    "            root.after(100, update_gui)\n",
    "        else:\n",
    "            print(\"No matching cases found.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        stop_progress_bar()\n",
    "\n",
    "def update_gui():\n",
    "    try:\n",
    "        while not result_queue.empty():\n",
    "            content = result_queue.get()\n",
    "            display_content(content)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while updating GUI: {e}\")\n",
    "    finally:\n",
    "        root.after(100, update_gui)\n",
    "\n",
    "def display_content(content):\n",
    "    content_window = tk.Toplevel()\n",
    "    content_window.title(\"Extracted Entities and Case Matches\")\n",
    "    text_area = Text(content_window, wrap='word')\n",
    "\n",
    "    content_to_display = \"Matched Cases:\\n\\n\"\n",
    "    for case in content:\n",
    "        content_to_display += f\"Case Name: {case[0]}\\nCase Details: {case[1]}\\n\\n\"\n",
    "\n",
    "    text_area.insert('1.0', content_to_display)\n",
    "    text_area.config(state='disabled')\n",
    "    text_area.pack(expand=True, fill='both')\n",
    "    content_window.geometry(\"600x500\")\n",
    "\n",
    "def extract_pdf(file_path):\n",
    "    print(f\"Processing PDF: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    content += text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "    return content.strip()\n",
    "\n",
    "def extract_docx(file_path):\n",
    "    print(f\"Processing DOCX: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            content += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOCX: {e}\")\n",
    "    return content.strip()\n",
    "\n",
    "def extract_txt(file_path):\n",
    "    print(f\"Processing TXT: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TXT: {e}\")\n",
    "    return content.strip()\n",
    "\n",
    "def extract_entities(text):\n",
    "    try:\n",
    "        entities = ner_pipeline(text)\n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "def start_progress_bar():\n",
    "    progress_bar.pack(pady=10)\n",
    "    progress_bar.start(10)\n",
    "\n",
    "def stop_progress_bar():\n",
    "    progress_bar.stop()\n",
    "    progress_bar.pack_forget()\n",
    "\n",
    "# Initialize the Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"File Upload and Content Extractor\")\n",
    "root.geometry(\"600x400\")\n",
    "root.config(bg=\"#f0f0f0\")\n",
    "instruction_label = tk.Label(root, text=\"Upload the case file as .pdf, .docx or .txt\",\n",
    "                               bg=\"#f0f0f0\", font=(\"Arial\", 12))\n",
    "instruction_label.pack(pady=10)\n",
    "\n",
    "# File label widget definition\n",
    "file_label = tk.Label(root, text=\"Selected File Path: \", bg=\"#f0f0f0\", font=(\"Arial\", 10))\n",
    "file_label.pack(pady=10)\n",
    "\n",
    "upload_btn = tk.Button(root, text=\"Upload File\", command=upload_file, bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12), height=2)\n",
    "upload_btn.pack(pady=10)\n",
    "\n",
    "search_btn = tk.Button(root, text=\"Search\", command=upload_file, bg=\"#2196F3\", fg=\"white\", font=(\"Arial\", 12), height=2)\n",
    "search_btn.pack(pady=10)\n",
    "\n",
    "progress_bar = ttk.Progressbar(root, mode='indeterminate')\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e846d793-2c24-411b-b030-6b22ac59ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: C:/Users/ASUS/OneDrive/Documents/Semester 9/Mini Project/Case File.pdf\n",
      "Extracted Entities: [(', 560001 - Model of Phone Purchased : iPhone 14 Pro - Price Paid : â‚¹1, 20, 000 - Receipt Number : 789456123 Upon purchasing the phone,', 'LABEL_26'), ('15, 2024', 'LABEL_15'), ('##nagar,', 'LABEL_26'), ('. Case Number : 2024 - CR - 1567 Date Filed :', 'LABEL_26'), ('##hakar. rao @ example. com Defendant Information : - Name :', 'LABEL_26'), ('Karnataka', 'LABEL_3'), ('California', 'LABEL_3'), ('21, 202', 'LABEL_15'), (', 560022 - Contact Number : + 91 98765 43210 - Email : p', 'LABEL_26'), ('Inc', 'LABEL_18'), ('Cup', 'LABEL_3'), ('##ino', 'LABEL_16'), ('Indira', 'LABEL_3'), ('##4 Court : District Consumer Disputes Redressal Forum,', 'LABEL_26'), ('USA', 'LABEL_3'), ('- Purchase Location : iRetail Store, 12, MG Road,', 'LABEL_26'), ('P', 'LABEL_8'), ('##rab', 'LABEL_6'), ('Complainant Information : - Name :', 'LABEL_26'), ('##ert', 'LABEL_26'), ('India', 'LABEL_3'), ('Apple', 'LABEL_5'), ('Ko', 'LABEL_3'), (', 560038 - Contact Number : + 91 80 4045 1000 - Email : support. in @ apple. com CASE SUMMARY : Nature of Complaint :', 'LABEL_26'), ('##ttigepal', 'LABEL_26'), ('alleges that he purchased an iPhone from an authorized Apple retailer in', 'LABEL_26'), ('., No. 7, 1st Floor, Abhiman Plaza, 2nd Main Road,', 'LABEL_26'), ('October', 'LABEL_2'), ('. - Address : 1, Apple Park Way,', 'LABEL_26'), ('Bangalore', 'LABEL_3'), (\"noticed several discrepancies, including : 1. Substandard Build Quality : The phone felt unusually lightweight, and the materials used did not match Apple ' s standards. 2. Software Issues : The operating system frequently crashed, and several applications failed to run. 3. Battery Performance : The battery drained rapidly, much faster than expected for a new iPhone. Actions Taken : - Prabhakar contacted the retailer to request a refund or exchange, but the retailer refused, claiming the device was genuine. - He subsequently contacted Apple â€™ s customer support, who confirmed that the serial number associated with the device did not match their records. Legal Claims : - Violation\", 'LABEL_26'), ('- Age : 34 - Gender : Male - Address : 45, Blossom Apartments, 3rd Main Road,', 'LABEL_26'), ('August', 'LABEL_2'), ('Case File :', 'LABEL_26'), ('. Upon using the device, he discovered it was a counterfeit product that did not function as promised and lacked the quality associated with genuine Apple produ cts. Details of Incident : - Purchase Date :', 'LABEL_26'), ('India Pvt. Ltd', 'LABEL_18'), ('Prab', 'LABEL_6'), (',', 'LABEL_26'), ('##ya', 'LABEL_16'), ('##hakar Rao', 'LABEL_19'), ('- Indian Office Address :', 'LABEL_26'), ('##rabhakar Rao vs. Apple Inc', 'LABEL_21')]\n",
      "Found Case Links: []\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "import queue\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, Text\n",
    "import logging\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = r\"C:\\Users\\ASUS\\OneDrive\\Documents\\Semester 9\\Mini Project\\Cases.csv\"\n",
    "cases_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define the tokenizer and model for NER (pre-trained or fine-tuned model)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./ner_model')\n",
    "model = BertForTokenClassification.from_pretrained('./ner_model')\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# Function to search for relevant cases from the dataset\n",
    "def search_cases_from_dataset(query):\n",
    "    filtered_cases = cases_df[cases_df.apply(\n",
    "        lambda row: row.astype(str).str.contains(query, case=False, na=False).any(), axis=1\n",
    "    )]\n",
    "    \n",
    "    case_links = []\n",
    "    for _, row in filtered_cases.iterrows():\n",
    "        case_links.append({\n",
    "            'Case_Title': row['Case_Title'],\n",
    "            'Citation': row['Citation'],\n",
    "            'Court': row['Court'],\n",
    "            'Judgement_Date': row['Judgement_Date'],\n",
    "            'Judges': row['Judges'],\n",
    "            'Petitioner': row['Petitioner'],\n",
    "            'Respondent': row['Respondent'],\n",
    "            'Bench': row['Bench'],\n",
    "            'Legal_Laws': row['Legal_Laws'],\n",
    "            'Preceding': row['Preceding'],\n",
    "            'Result': row['Result']\n",
    "        })\n",
    "    \n",
    "    return case_links\n",
    "\n",
    "\n",
    "# Function to process the uploaded case files (PDF, DOCX, TXT)\n",
    "def upload_file():\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Case File\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\"), (\"Word Documents\", \"*.docx\"), (\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        threading.Thread(target=process_file, args=(file_path,), daemon=True).start()\n",
    "        start_progress_bar()\n",
    "    else:\n",
    "        messagebox.showinfo(\"Info\", \"No file selected.\")\n",
    "\n",
    "# Function to process the file and print necessary outputs\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        logging.info(f\"File Selected: {file_path}\")\n",
    "        file_label.config(text=f\"Selected File Path: {file_path}\")\n",
    "\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            content = extract_pdf(file_path)\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            content = extract_docx(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            content = extract_txt(file_path)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Unsupported file format.\")\n",
    "            return\n",
    "\n",
    "        entities = extract_entities(content)\n",
    "        unique_entities = list({(ent['word'], ent['entity_group']) for ent in entities})\n",
    "        print(\"Extracted Entities:\", unique_entities)\n",
    "\n",
    "        search_query = \" \".join([ent[0] for ent in unique_entities])\n",
    "        case_links = search_cases_from_dataset(search_query)  # Search in dataset instead of Indian Kanoon\n",
    "        print(\"Found Case Links:\", case_links)\n",
    "\n",
    "        # Display extracted entities and case links\n",
    "        for i in range(min(len(unique_entities), len(case_links))):\n",
    "            ent = unique_entities[i]\n",
    "            link = case_links[i]\n",
    "            print(f\"{ent[0]} : {link}\")\n",
    "\n",
    "        result_queue.put(content)\n",
    "        root.after(100, update_gui)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        stop_progress_bar()\n",
    "\n",
    "# Update the GUI with the extracted content\n",
    "def update_gui():\n",
    "    try:\n",
    "        while not result_queue.empty():\n",
    "            content = result_queue.get()\n",
    "            display_content(content)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while updating GUI: {e}\")\n",
    "    finally:\n",
    "        root.after(100, update_gui)\n",
    "\n",
    "# Display the extracted content in a new window\n",
    "def display_content(content):\n",
    "    content_window = tk.Toplevel()\n",
    "    content_window.title(\"Extracted Content\")\n",
    "    text_area = Text(content_window, wrap='word')\n",
    "    text_area.insert('1.0', \"Extracted Content:\\n\\n\" + content + \"\\n\\n\")\n",
    "    text_area.config(state='disabled')\n",
    "    text_area.pack(expand=True, fill='both')\n",
    "    content_window.geometry(\"600x500\")\n",
    "\n",
    "# Extract content from PDF files\n",
    "def extract_pdf(file_path):\n",
    "    print(f\"Processing PDF: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    content += text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "    return content.strip()\n",
    "\n",
    "# Extract content from DOCX files\n",
    "def extract_docx(file_path):\n",
    "    print(f\"Processing DOCX: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            content += paragraph.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DOCX: {e}\")\n",
    "    return content.strip()\n",
    "\n",
    "# Extract content from TXT files\n",
    "def extract_txt(file_path):\n",
    "    print(f\"Processing TXT: {file_path}\")\n",
    "    content = \"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TXT: {e}\")\n",
    "    return content.strip()\n",
    "\n",
    "# Extract entities using the fine-tuned NER model\n",
    "def extract_entities(text):\n",
    "    try:\n",
    "        entities = ner_pipeline(text)\n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return []\n",
    "\n",
    "# Start the progress bar\n",
    "def start_progress_bar():\n",
    "    progress_bar.pack(pady=10)\n",
    "    progress_bar.start(10)\n",
    "\n",
    "# Stop the progress bar\n",
    "def stop_progress_bar():\n",
    "    progress_bar.stop()\n",
    "    progress_bar.pack_forget()\n",
    "\n",
    "# Initialize the Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"File Upload and Content Extractor\")\n",
    "root.geometry(\"600x400\")\n",
    "root.config(bg=\"#f0f0f0\")\n",
    "instruction_label = tk.Label(root, text=\"Upload the case file as .pdf, .docx or .txt\",\n",
    "                               bg=\"#f0f0f0\", font=(\"Arial\", 12))\n",
    "instruction_label.pack(pady=10)\n",
    "\n",
    "# Entry for search term\n",
    "search_entry = tk.Entry(root, width=50, font=(\"Arial\", 12))\n",
    "search_entry.pack(pady=10)\n",
    "\n",
    "# Search button\n",
    "search_btn = tk.Button(root, text=\"Search\", command=upload_file, width=20, bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "search_btn.pack(pady=10)\n",
    "\n",
    "# Frame for the upload button\n",
    "frame = tk.Frame(root, bg=\"#ffffff\", bd=2, relief=tk.GROOVE)\n",
    "frame.pack(pady=20, padx=10, fill='both', expand=True)\n",
    "upload_btn = tk.Button(frame, text=\"Upload File\", command=upload_file, width=20, bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "upload_btn.pack(pady=10)\n",
    "\n",
    "# Label to display selected file path\n",
    "file_label = tk.Label(frame, text=\"Selected File Path: \", bg=\"#f0f0f0\", font=(\"Arial\", 10))\n",
    "file_label.pack(pady=5)\n",
    "\n",
    "# Progress bar (initially hidden)\n",
    "progress_bar = ttk.Progressbar(root, orient=\"horizontal\", mode=\"indeterminate\")\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcefb2e-fa36-4361-a3e8-e7acdae47351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
